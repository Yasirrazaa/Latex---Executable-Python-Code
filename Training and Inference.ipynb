{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae642c8-dc50-4405-bb40-721384e0c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME is set to: /workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace'\n",
    "print(\"HF_HOME is set to:\", os.getenv('HF_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c873df-6000-4fe8-be7f-c7ec88f2ff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm_bootcamp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02423b2a-41dc-4aa5-b2be-144b7726b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 17:51:18,553 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2024-08-26 17:51:31,568 - INFO - Model loaded and evaluation mode set.\n",
      "Processing batches:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def inspect_json_data(file_path, num_samples=5):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Sample {i+1}: {data[i]}\")\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    def fix_data_types(item):\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, float):\n",
    "                item[key] = str(value)  # Convert floats to strings (if applicable)\n",
    "            elif isinstance(value, list):\n",
    "                # Recursively handle lists\n",
    "                item[key] = [str(v) if isinstance(v, float) else v for v in value]\n",
    "            elif isinstance(value, dict):\n",
    "                # Recursively handle nested dictionaries\n",
    "                item[key] = fix_data_types(value)\n",
    "        return item\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        data=data[:16]\n",
    "        # Ensure all items are dictionaries and fix data types\n",
    "        return [fix_data_types(item) for item in data if isinstance(item, dict)]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        \n",
    "\n",
    "def extract_variables(input_values):\n",
    "    filtered_keys = [key for key in input_values.keys()]\n",
    "    return filtered_keys\n",
    "\n",
    "def generate_python_code_batch(pipe, latex_expressions, variables_list):\n",
    "    prompts = [f\"\"\"Convert the following LaTeX expression into a Python function named 'calculate' that takes the given parameters:\n",
    "{latex_expression}\n",
    "\n",
    "Follow these guidelines strictly:\n",
    "1. The code must contain a Python function definition using `def calculate({', '.join(variables)}):` for all equations including derivatives. Import necessary libraries and functions at the beginning of the function. Prefer using `numpy` for mathematical operations and `SymPy` for symbolic computations.\n",
    "2. Use `SymPy` exclusively to solve equations involving calculus operations like derivative, differentiation, integration, and logarithmic functions. For example, if the expression involves differentiation, compute the derivative directly inside the function. Do not use `sp.lambdify`.\n",
    "3. Ensure the function is callable with the provided parameters and returns the calculated result as `int`, `float`, or `complex number` with 'return'.\n",
    "\n",
    "Now, apply the above instructions method to convert the given LaTeX expression.\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        for latex_expression, variables in zip(latex_expressions, variables_list)\n",
    "    ]\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        outputs = pipe(prompts, **gen_config)\n",
    "        results = []\n",
    "        for output, latex_expression in zip(outputs, latex_expressions):\n",
    "            generated_text = output[0][\"generated_text\"]\n",
    "            python_code = re.findall(r\"```python(.*?)```\", generated_text, re.DOTALL)\n",
    "            if not python_code:\n",
    "                logger.error(f\"No Python code generated for LaTeX expression: {latex_expression}\")\n",
    "                results.append((None, \"No Python code generated\"))\n",
    "            else:\n",
    "                results.append((python_code[0].strip(), None))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Python code for batch: {e}\")\n",
    "        return [(None, f\"Error generating Python code: {str(e)}\") for _ in latex_expressions]\n",
    "\n",
    "\n",
    "def process_json_data_batch(pipe, json_data_list, batch_size=8):\n",
    "    results = []\n",
    "    total_batches = (len(json_data_list) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "    for i in tqdm(range(0, len(json_data_list), batch_size), total=total_batches, desc=\"Processing batches\"):\n",
    "        batch = json_data_list[i:i+batch_size]\n",
    "        \n",
    "        latex_expressions = []\n",
    "        variables_list = []\n",
    "        task_ids = []\n",
    "\n",
    "        for json_data in batch:\n",
    "            task_id = json_data.get(\"task_id\", \"unknown\")\n",
    "            latex_expression = json_data.get(\"latex_expression\")\n",
    "            test_cases = json_data.get(\"test_cases\", [])\n",
    "\n",
    "            if not latex_expression:\n",
    "                logger.warning(f\"Task ID {task_id}: No LaTeX expression provided\")\n",
    "                results.append((task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No LaTeX expression provided\"}]))\n",
    "                continue\n",
    "\n",
    "            if not test_cases:\n",
    "                logger.warning(f\"Task ID {task_id}: No test cases provided\")\n",
    "                results.append((task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No test cases provided\"}]))\n",
    "                continue\n",
    "\n",
    "            variables = extract_variables(test_cases[0].get(\"input\", {}))\n",
    "            \n",
    "            latex_expressions.append(latex_expression)\n",
    "            variables_list.append(variables)\n",
    "            task_ids.append(task_id)\n",
    "\n",
    "        if latex_expressions:\n",
    "            batch_results = generate_python_code_batch(pipe, latex_expressions, variables_list)\n",
    "            for task_id, latex_expression, (python_code, error) in zip(task_ids, latex_expressions, batch_results):\n",
    "                if error:\n",
    "                    logger.error(f\"Task ID {task_id}: {error}\")\n",
    "                    results.append((task_id, [{\"latex_expression\": latex_expression, \"error\": error}]))\n",
    "                else:\n",
    "                    results.append((task_id, [{\"latex_expression\": latex_expression, \"generated_code\": python_code}]))\n",
    "\n",
    "    return results\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    try:\n",
    "        model_name = './fine_tuned_aimo_lora_model_v3'\n",
    "        # Load the model\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, tokenizer='./full_latest_v3_tokenizer', torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        pipe.model.eval()\n",
    "        logger.info(\"Model loaded and evaluation mode set.\")\n",
    "\n",
    "        # Load JSON data\n",
    "        json_data_list = load_json_file(input_file)\n",
    "\n",
    "        # Initialize a dictionary to store results\n",
    "        results_dict = {\n",
    "            \"id\": [],\n",
    "            \"latex_expression\": [],\n",
    "            \"generated_code\": [],\n",
    "            \"error\": []\n",
    "        }\n",
    "\n",
    "        # Process JSON objects in batches and store results in the dictionary\n",
    "        batch_size = 8  # You can adjust this value based on your GPU memory\n",
    "        results = process_json_data_batch(pipe, json_data_list, batch_size)\n",
    "\n",
    "        logger.info(\"Processing results...\")\n",
    "        for task_id, task_results in tqdm(results, desc=\"Storing results\"):\n",
    "            for result in task_results:\n",
    "                results_dict[\"id\"].append(task_id)\n",
    "                results_dict[\"latex_expression\"].append(result.get(\"latex_expression\", \"\"))\n",
    "                results_dict[\"generated_code\"].append(result.get(\"generated_code\", \"\"))\n",
    "                results_dict[\"error\"].append(result.get(\"error\", \"\"))\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "        df = pd.DataFrame(results_dict)\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        logger.info(f\"Results have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "input_file = 'public_test_new_no_sol_no_out.json'\n",
    "output_file = 'codes_26_3.csv'\n",
    "main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e83c36-f363-4d28-a031-50dd458a7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def inspect_json_data(file_path, num_samples=5):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Sample {i+1}: {data[i]}\")\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    def fix_data_types(item):\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, float):\n",
    "                item[key] = str(value)  # Convert floats to strings (if applicable)\n",
    "            elif isinstance(value, list):\n",
    "                # Recursively handle lists\n",
    "                item[key] = [str(v) if isinstance(v, float) else v for v in value]\n",
    "            elif isinstance(value, dict):\n",
    "                # Recursively handle nested dictionaries\n",
    "                item[key] = fix_data_types(value)\n",
    "        return item\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        data=data[:8]\n",
    "        # Ensure all items are dictionaries and fix data types\n",
    "        return [fix_data_types(item) for item in data if isinstance(item, dict)]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        \n",
    "\n",
    "def extract_variables(input_values):\n",
    "    filtered_keys = [key for key in input_values.keys()]\n",
    "    return filtered_keys\n",
    "\n",
    "def generate_python_code(pipe, latex_expression, variables):\n",
    "    prompt = f\"\"\"\n",
    "Convert this LaTeX expression to a Python function named 'def calculate' that takes the following parameters: {','.join(variables)}\n",
    "Latex: {latex_expression}\n",
    "Just Give me a python function with all necessary libraries and functions used and put all code inside the function. The function must give output in integer or float or complex number.\n",
    "1. Use `numpy` for general mathematical operations (e.g., `1/np.tan` instead of `np.cot`).\n",
    "2. Use `sympy` for logarithms, calculus, differentiation, and integrals. Handle complex numbers if they appear.\n",
    "3.Use python and sympy to define the function directly.\n",
    "\n",
    "Python Code:\n",
    "\"\"\"\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 2000,  # Increased token limit\n",
    "        \"do_sample\": False,\n",
    "        \"stop_strings\": [\"```output\"],\n",
    "        \"tokenizer\": pipe.tokenizer,\n",
    "    }\n",
    "    try:\n",
    "        outputs = pipe(prompt, **gen_config)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "        # Print the raw output for debugging\n",
    "        #print(f\"Raw generated text for LaTeX expression '{latex_expression}':\\n{generated_text}\\n\")\n",
    "\n",
    "        python_code = re.findall(r\"```python(.*?)```\", generated_text, re.DOTALL)\n",
    "        if not python_code:\n",
    "            return None, \"No Python code generated\"\n",
    "        return python_code[0].strip(), None\n",
    "    except Exception as e:\n",
    "        return None, f\"Error generating Python code: {str(e)}\"\n",
    "\n",
    "def process_json_data(pipe, json_data):\n",
    "    task_id = json_data.get(\"task_id\", \"unknown\")\n",
    "    latex_expression = json_data.get(\"latex_expression\")\n",
    "    test_cases = json_data.get(\"test_cases\", [])\n",
    "\n",
    "    print(f\"Processing task ID: {task_id}\")\n",
    "\n",
    "    if not latex_expression:\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No LaTeX expression provided\"}]\n",
    "\n",
    "    if not test_cases:\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No test cases provided\"}]\n",
    "\n",
    "    if not isinstance(test_cases[0], dict):\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: Test case is not a dictionary\"}]\n",
    "\n",
    "    variables = extract_variables(test_cases[0].get(\"input\", {}))\n",
    "    python_code, error = generate_python_code(pipe, latex_expression, variables)\n",
    "    if error:\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": error}]\n",
    "\n",
    "    return task_id, [{\"latex_expression\": latex_expression, \"generated_code\": python_code}]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return [{k: v for k, v in item.items() if v is not None} for item in batch if item]\n",
    "\n",
    "def process_batch(pipe, batch):\n",
    "    results = []\n",
    "    for item in batch:\n",
    "        task_id, result = process_json_data(pipe, item)\n",
    "        results.extend([(task_id, r) for r in result])\n",
    "    return results\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    try:\n",
    "        model_name = './fine_tuned_aimo_lora_model_v2'\n",
    "\n",
    "\n",
    "        pipe = pipeline(\"text-generation\", model=model_name,torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        pipe.model.eval()\n",
    "\n",
    "        # Load JSON data and fix types\n",
    "        json_data_list = load_json_file(input_file)\n",
    "\n",
    "        # Create a Hugging Face Dataset\n",
    "        dataset = Dataset.from_list(json_data_list)\n",
    "\n",
    "        # Create a DataLoader with custom collate function\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "        results_dict = {\n",
    "            \"id\": [],\n",
    "            \"latex_expression\": [],\n",
    "            \"generated_code\": [],\n",
    "            \"error\": []\n",
    "        }\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            batch_results = process_batch(pipe, batch)\n",
    "            for task_id, result in batch_results:\n",
    "                results_dict[\"id\"].append(task_id)\n",
    "                results_dict[\"latex_expression\"].append(result.get(\"latex_expression\", \"\"))\n",
    "                results_dict[\"generated_code\"].append(result.get(\"generated_code\", \"\"))\n",
    "                results_dict[\"error\"].append(result.get(\"error\", \"\"))\n",
    "\n",
    "        df = pd.DataFrame(results_dict)\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Results have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "input_file = 'public_test_new_no_sol_no_out.json'\n",
    "output_file = 'codes_25_2.csv'\n",
    "main(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a663f-0764-43c6-b89e-e1c82ff7a2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b42f5da-a708-48e6-9c49-7cccc0107c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Problem: Convert this LaTeX expression '\\mathtt{\\text{Derivative(a*x + b + x**2 + 4*x + sqrt(a + exp(x)) + 3, x)}}' to a Python function named 'calculate'. Just Give me a callable function with all necessary libraries and functions used and put all code inside the function. The function must give output in integer or float.\n",
      "### Solution: \n",
      "To solve the problem, we need to convert the given LaTeX expression into a Python function that can be called. The expression is a derivative calculation involving symbolic variables and functions.\n",
      "\n",
      "The given LaTeX expression is:\n",
      "\\[ \\text{Derivative(a*x + b + x**2 + 4*x + \\sqrt{a + \\exp(x)} + 3, x)} \\]\n",
      "\n",
      "We will use the `sympy` library in Python to perform the differentiation. Here's the step-by-step process:\n",
      "\n",
      "1. **Import the necessary libraries:**\n",
      "   - `sympy` is a Python library for symbolic mathematics.\n",
      "\n",
      "2. **Define the necessary symbols and functions:**\n",
      "   - We will define the symbols `a`, `b`, and `x`.\n",
      "   - We will also define the function `sqrt` and `exp` from `sympy`.\n",
      "\n",
      "3. **Define the expression to differentiate:**\n",
      "   - The expression is `a*x + b + x**2 + 4*x + sqrt(a + exp(x)) + 3`.\n",
      "\n",
      "4. **Differentiate the expression with respect to `x`:**\n",
      "   - Use the `diff` function from `sympy` to compute the derivative.\n",
      "\n",
      "5. **Create a Python function to calculate the derivative:**\n",
      "   - The function will take the parameters `a` and `b` as inputs.\n",
      "\n",
      "Here's the Python code to achieve this:\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# Define the symbols\n",
      "a, b, x = sp.symbols('a b x')\n",
      "\n",
      "# Define the expression\n",
      "expression = a*x + b + x**2 + 4*x + sp.sqrt(a + sp.exp(x)) + 3\n",
      "\n",
      "# Compute the derivative with respect to x\n",
      "derivative = sp.diff(expression, x)\n",
      "\n",
      "# Define the function to calculate the derivative\n",
      "def calculate_derivative(a_val, b_val):\n",
      "    # Substitute the values of a and b into the derivative\n",
      "    derivative_substituted = derivative.subs({a: a_val, b: b_val})\n",
      "    # Return the result\n",
      "    return derivative_substituted\n",
      "\n",
      "# Example: Calculate the derivative for a given a and b\n",
      "a_val = 2\n",
      "b_val = 3\n",
      "result = calculate_derivative(a_val, b_val)\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "2*x + 6 + exp(x)/(2*sqrt(exp(x) + 2))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"./fine_tuned_aimo_lora_model_v3\",tokenizer='AI-MO/NuminaMath-7B-TIR', torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Convert this LaTeX expression '\\\\mathtt{\\\\text{Derivative(a*x + b + x**2 + 4*x + sqrt(a + exp(x)) + 3, x)}}' to a Python function named 'calculate'. Just Give me a callable function with all necessary libraries and functions used and put all code inside the function. The function must give output in integer or float.\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "gen_config = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": False,\n",
    "    \"stop_strings\": [\"```output\"], # Generate until Python code block is complete\n",
    "    \"tokenizer\": pipe.tokenizer,\n",
    "}\n",
    "\n",
    "outputs = pipe(prompt, **gen_config)\n",
    "text = outputs[0][\"generated_text\"]\n",
    "print(text)\n",
    "\n",
    "\n",
    "python_code = re.findall(r\"```python(.*?)```\", text, re.DOTALL)[0]\n",
    "exec(python_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77fccc-7ef7-4040-a39e-3084d9c111f7",
   "metadata": {},
   "source": [
    "# Testing on Competition Data without Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45db615-07b3-4d14-8bbd-2b11af76d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def extract_variables(input_values):\n",
    "    return input_values.keys()\n",
    "\n",
    "def generate_python_code(pipe, latex_expression, variables):\n",
    "    prompt = f\"\"\"Convert this LaTeX expression to a Python function named 'calculate' that takes the following parameters: {', '.join(variables)}:\n",
    "{latex_expression}\n",
    "\n",
    "The function should evaluate the expression and return a numerical result as a float or an integer.\n",
    "Use math functions from the math module (e.g., math.sin, math.cos) for trigonometric functions.\n",
    "For inverse trigonometric functions, use math.asin, math.acos, math.atan.\n",
    "For hyperbolic functions, use math.sinh, math.cosh, math.tanh.\n",
    "For logarithms, use math.log for natural log and math.log10 for base 10 log.\n",
    "Use math.pi for π and math.e for e.\n",
    "For complex numbers, use the cmath module.\n",
    "\n",
    "Python function:\"\"\"\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": False,\n",
    "        \"stop_strings\": [\"```output\"],\n",
    "        \"tokenizer\": pipe.tokenizer,\n",
    "    }\n",
    "    try:\n",
    "        outputs = pipe(prompt, **gen_config)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        python_code = re.findall(r\"```python(.*?)```\", generated_text, re.DOTALL)\n",
    "        if not python_code:\n",
    "            return None, \"No Python code generated\"\n",
    "        return python_code[0].strip(), None\n",
    "    except Exception as e:\n",
    "        return None, f\"Error generating Python code: {str(e)}\"\n",
    "\n",
    "\n",
    "def execute_python_code(code, input_values):\n",
    "    try:\n",
    "        local_scope = {}\n",
    "        exec(code, globals(), local_scope)\n",
    "        if 'calculate' not in local_scope:\n",
    "            return 0\n",
    "        \n",
    "        # Dynamically pass all input variables to the calculate function\n",
    "        result = local_scope['calculate'](**input_values)\n",
    "        \n",
    "        # Ensure that the result is evaluated to a number\n",
    "        if isinstance(result, str):\n",
    "            result = eval(result)\n",
    "\n",
    "        if isinstance(result, complex):\n",
    "            return f\"{result.real}+{result.imag}j\"\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def process_json_data(pipe, json_data):\n",
    "    task_id = json_data.get(\"task_id\", \"unknown\")\n",
    "    latex_expression = json_data.get(\"latex_expression\")\n",
    "    test_cases = json_data.get(\"test_cases\", [])\n",
    "\n",
    "    if not latex_expression:\n",
    "        return task_id, json.dumps([\"Error: No LaTeX expression provided\"])\n",
    "\n",
    "    # Use the input variables from the first test case to generate the function\n",
    "    if not test_cases:\n",
    "        return task_id, json.dumps([\"Error: No test cases provided\"])\n",
    "\n",
    "    # Extract variables from the first test case input\n",
    "    variables = extract_variables(test_cases[0].get(\"input\", {}))\n",
    "\n",
    "    python_code, error = generate_python_code(pipe, latex_expression, variables)\n",
    "    if error:\n",
    "        return task_id, json.dumps([error])\n",
    "\n",
    "    results = []\n",
    "    for case in test_cases:\n",
    "        input_values = case.get(\"input\", {})\n",
    "        result = execute_python_code(python_code, input_values)\n",
    "        results.append(result)\n",
    "\n",
    "    return task_id, json.dumps(results)\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    try:\n",
    "        # Load the model\n",
    "        pipe = pipeline(\"text-generation\", model=\"AI-MO/NuminaMath-7B-TIR\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "        # Load JSON data\n",
    "        json_data_list = load_json_file(input_file)\n",
    "\n",
    "        # Process each JSON object and store results\n",
    "        results = {\n",
    "        \"id\":[],\n",
    "        \"outputs\":[]\n",
    "        }\n",
    "        for json_data in json_data_list:\n",
    "            task_id, outputs = process_json_data(pipe, json_data)\n",
    "            results['id'].append(f\"{task_id}\")\n",
    "            results['outputs'].append(f\"{outputs}\")\n",
    "\n",
    "        # Write results to a file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"id,outputs\\n\")\n",
    "            for result in results:\n",
    "                f.write(f\"{result}\\n\")\n",
    "\n",
    "        print(f\"Results have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "input_file = 'public_test_new_no_sol_no_out.json'\n",
    "output_file = 'sub.csv'\n",
    "main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ca6a7-3646-47ba-bcca-4791537c4866",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f318697-43ce-4800-a13f-bf60a6cbc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72428b92-5d33-4a1e-8ef6-f4a5367ef78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_and_prepare_data_from_csv(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Prepare the conversations list\n",
    "    conversations = []\n",
    "    for _, row in df.iterrows():\n",
    "        latex_expression = row['latex_expression'].strip()\n",
    "        solution = row['solution'].strip()\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": latex_expression},\n",
    "            {\"role\": \"assistant\", \"content\": solution}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    # Convert the conversations list to a Dataset\n",
    "    return Dataset.from_dict({\"conversations\": conversations})\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_and_prepare_data_from_csv('synthetic_data_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985c693-b9e3-44fc-93e9-50c73ace500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your provided function to load and prepare data\n",
    "def load_and_prepare_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    problems = content.split('==================================================')\n",
    "    problems = [p.strip() for p in problems if p.strip()]\n",
    "    \n",
    "    conversations = []\n",
    "    for problem in problems:\n",
    "        parts = problem.split('Python Solution:')\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        \n",
    "        input_text = parts[0].strip()\n",
    "        solution = parts[1].strip().replace('```python', '').replace('```', '').strip()\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "            {\"role\": \"assistant\", \"content\": solution}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return Dataset.from_dict({\"conversations\": conversations})\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_and_prepare_data('textbook_format_v2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602c09b-0d1f-417a-8819-f9e510c4c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Step 1: Load and prepare the data\n",
    "def load_and_prepare_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    conversations = []\n",
    "    for item in data:\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": f\"Convert the following LaTeX expression to Python code:\\nExpression: {item['sympy_exp']}\\nLaTeX: {item['latex_expression']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\" {item['solution']} \\n Simplified Solution: {item['simplified_solution']}\"}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return Dataset.from_dict({\"conversations\": conversations})\n",
    "\n",
    "# Load your data\n",
    "dataset = load_and_prepare_data('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed5b04-d64e-4117-a76e-068fbd8ae4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_tokenizer = \"AI-MO/NuminaMath-7B-TIR\"\n",
    "model_name=\"./fine_tuned_aimo_lora_model_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Get the PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Ensure the tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a8762-3cbf-4e36-9b84-3dc593696e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in training mode\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    model_inputs = tokenizer.batch_encode_plus(\n",
    "        [tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True) for conv in examples['conversations']],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels by copying input_ids\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask the prompt part in labels\n",
    "    for i, conversation in enumerate(examples['conversations']):\n",
    "        prompt = tokenizer.encode(conversation[0][\"content\"])\n",
    "        prompt_length = len(prompt)\n",
    "        model_inputs[\"labels\"][i, :prompt_length] = -100\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(prepare_train_features, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a6ccb-9997-43ef-b874-e729891e0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./aimo\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,  # Use fp16 mixed precision\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#model.to_empty(device=training_args.device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93768f-3012-4262-96d3-55380e51a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b593f9-4548-467b-bb22-60d28ecba9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_aimo_lora_model_v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8dbca-8532-4921-b844-821eb26b18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_path = './fine_tuned_aimo_lora_model_v3'\n",
    "tokenizer_path = './full_latest_72_tokenizer'\n",
    "\n",
    "# Load the model and tokenizer from their respective paths\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Now create the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "# Save the model and tokenizer to the desired locations\n",
    "pipe.model.save_pretrained('./full_latest_model_v3')\n",
    "pipe.tokenizer.save_pretrained('./full_latest_v3_tokenizer')\n",
    "\n",
    "print(pipe.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69c397-e929-46f5-907c-97118460466d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa427f9-b17e-41b8-ad4c-96151bab1d3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      5\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine_tuned_aimo_lora_model_v3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_name, \n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      9\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained('./full_latest_v3_tokenizer')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name='./fine_tuned_aimo_lora_model_v3'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('./full_latest_v3_tokenizer')\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Convert this LaTeX expression to a Python function named 'calculate' that takes the following parameters: x:\n",
    "{{3 \\tan{\\left(8 x \\right)} + 5}}\n",
    "\n",
    "Follow these guidelines strictly:\n",
    "1. Import necessary libraries and functions at the beginning of the function. Prefer using `numpy` for mathematical operations and `SymPy` for symbolic computations.\n",
    "2. Use `SymPy` to solve the equations, including calculus operations like differentiation, integration, and logarithmic functions. Do not use `sp.lambdify` for defining functions.\n",
    "3. The function should return results as `int`, `float`, or `complex`. Use `SymPy` functions like `.evalf()` for numerical evaluation if needed.\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\"\"\"\n",
    "    }]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "gen_config = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": False,\n",
    "    \"stop_strings\": [\"```output\"], # Generate until Python code block is complete\n",
    "    \"tokenizer\": pipe.tokenizer,\n",
    "}\n",
    "\n",
    "outputs = pipe(prompt, **gen_config)\n",
    "text = outputs[0][\"generated_text\"]\n",
    "print(text)\n",
    "\n",
    "\n",
    "python_code = re.findall(r\"```python(.*?)```\", text, re.DOTALL)[0]\n",
    "exec(python_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da3069-790c-46c0-8a53-0ad811b31620",
   "metadata": {},
   "source": [
    "# Code Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2dd61-268a-4e6f-8b7b-fd0f9dcce141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "def run_code(compiled_code, func_name, test_case):\n",
    "    # Create a dictionary to execute the code in its own namespace\n",
    "    local_namespace = {}\n",
    "    \n",
    "    # Execute the compiled code\n",
    "    exec(compiled_code, globals(), local_namespace)\n",
    "    \n",
    "    # Capture the standard output\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        # Run the function from the compiled code with the test case as an argument\n",
    "        output = local_namespace[func_name](*test_case)\n",
    "    finally:\n",
    "        # Reset standard output and retrieve the result\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def check_correctness(output, expected_output, rel_tol=1e-9, abs_tol=0.0):\n",
    "    # Check if the outputs are numbers\n",
    "    if isinstance(output, (int, float)) and isinstance(expected_output, (int, float)):\n",
    "        # Use isclose for numerical comparison\n",
    "        return math.isclose(output, expected_output, rel_tol=rel_tol, abs_tol=abs_tol)\n",
    "    else:\n",
    "        # Fallback to equality check for non-numerical outputs\n",
    "        return output == expected_output\n",
    "\n",
    "def compile_code(code):\n",
    "    try:\n",
    "        # Compile the code string into a code object\n",
    "        compiled_code = compile(code, '<string>', 'exec')\n",
    "        \n",
    "        # Extract the function name(s) from the code (assuming there is only one top-level function)\n",
    "        local_namespace = {}\n",
    "        exec(compiled_code, globals(), local_namespace)\n",
    "        \n",
    "        # Get the function name by extracting keys from local_namespace that are callable (functions)\n",
    "        func_name = None\n",
    "        for name, obj in local_namespace.items():\n",
    "            if callable(obj):\n",
    "                func_name = name\n",
    "                break\n",
    "        \n",
    "        return compiled_code, func_name\n",
    "    \n",
    "    except Exception as e:\n",
    "        # In case of an error, return None and the error message\n",
    "        print(f\"Compilation failed: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73285d7-14d3-4a83-9b8a-5ac37018b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code(generated_code_list, test_cases, expected_outputs):\n",
    "    top_code = generated_code_list[0]\n",
    "    top_compiled_code, top_func_name = compile_code(top_code)\n",
    "\n",
    "    if top_compiled_code is None:\n",
    "        return 0.0  # Top Code not compiled successfully, so accuracy is 0.\n",
    "\n",
    "    correct_count = 0\n",
    "\n",
    "    # Calculate accuracy for the top generated code\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        output = run_code(top_compiled_code, top_func_name, test_case)\n",
    "        if check_correctness(output, expected_outputs[i]):\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / len(test_cases)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63397f28-75ba-42e3-81b6-58106dfa3f86",
   "metadata": {},
   "source": [
    "# Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "754181a0-c912-434a-80b5-c9b34705b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 15:57:11,152 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2024-08-26 15:57:21,309 - INFO - Model loaded and evaluation mode set.\n",
      " 67%|██████▋   | 676/1004 [51:26<30:29,  5.58s/it]  2024-08-26 16:48:58,949 - ERROR - No Python code generated for LaTeX expression: \\frac{9.48045640713735 e^{- 0.795612954929873 x}}{970.362343722415 \\pi r \\left(r + \\sqrt{h^{2} + r^{2}}\\right) + 1.49981800883714 v + 1.67078922044761 w^{2} + 0.548536816459822 y^{2} + 1.71408886030606 z^{2} + 2.50166367824986 e^{- 1.01197189210867 x}}\n",
      "2024-08-26 16:48:58,951 - ERROR - Task ID 970ed9f3: No Python code generated\n",
      " 92%|█████████▏| 919/1004 [1:14:39<07:10,  5.07s/it]2024-08-26 17:12:11,617 - ERROR - No Python code generated for LaTeX expression: \\frac{1.26415982655854 x + 5.22351587682023 y + 3 z^{3} - 7744 + \\sum_{x=1}^{5} 3^{x} + 1.18847493653838 e^{- 1.25360271338448 z}}{\\frac{7 w^{2}}{6 x + 8} + \\frac{5 x}{2 x + 5} + \\frac{7 y}{4 x + 6} + \\frac{6 z}{z + 4} - 0.5}\n",
      "2024-08-26 17:12:11,621 - ERROR - Task ID 8a106c2d: No Python code generated\n",
      " 95%|█████████▌| 956/1004 [1:18:19<04:51,  6.07s/it]2024-08-26 17:15:52,384 - ERROR - No Python code generated for LaTeX expression: \\left(1.39511896048849 v + 0.865112862746263 x^{2} + 0.887690256784152 y + 0.430279009198709 z^{2} + 5.3322452049551 e^{- 1.54325333870612 w}\\right) \\left(\\frac{5 w^{2}}{3 w + 6} + \\frac{8 x}{10 y + 2} + \\frac{7 y^{3}}{8 y + 8} + \\frac{z}{z + 4} - 2967.03669230411 \\log{\\left(1014.77325568273 x \\right)} - 262.344783566221\\right)\n",
      "2024-08-26 17:15:52,387 - ERROR - Task ID 5bd5230f: No Python code generated\n",
      "100%|██████████| 1004/1004 [1:23:00<00:00,  4.96s/it]\n",
      "2024-08-26 17:20:22,206 - INFO - Results have been written to codes_26_2.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def extract_variables(input_values):\n",
    "    filtered_keys = [key for key in input_values.keys()]\n",
    "\n",
    "    return filtered_keys\n",
    "\n",
    "def generate_python_code(pipe, latex_expression, variables):\n",
    "     \n",
    "    prompt = f\"\"\"Convert the following LaTeX expression into a Python function named 'calculate' that takes the given parameters:\n",
    "{latex_expression}\n",
    "\n",
    "Follow these guidelines strictly:\n",
    "1. The code must contain a Python function definition using `def calculate({', '.join(variables)}):` for all equations including derivatives. Import necessary libraries and functions at the beginning of the function. Prefer using `numpy` for mathematical operations and `SymPy` for symbolic computations.\n",
    "2. Use `SymPy` exclusively to solve equations involving calculus operations like differentiation, integration, and logarithmic functions. For example, if the expression involves differentiation, compute the derivative directly inside the function. Do not use `sp.lambdify`.\n",
    "3. Ensure the function is callable with the provided parameters and returns the calculated result as `int`, `float`, or `complex number` with 'return'.\n",
    "\n",
    "Now, apply the above instructions method to convert the given LaTeX expression.\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"stop_strings\": [\"```output\"],\n",
    "        \"tokenizer\": pipe.tokenizer,\n",
    "    }\n",
    "    try:\n",
    "        outputs = pipe(prompt, **gen_config)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        python_code = re.findall(r\"```python(.*?)```\", generated_text, re.DOTALL)\n",
    "        if not python_code:\n",
    "            logger.error(f\"No Python code generated for LaTeX expression: {latex_expression}\")\n",
    "            return None, \"No Python code generated\"\n",
    "        return python_code[0].strip(), None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Python code for LaTeX expression {latex_expression}: {e}\")\n",
    "        return None, f\"Error generating Python code: {str(e)}\"\n",
    "\n",
    "def process_json_data(pipe, json_data):\n",
    "    task_id = json_data.get(\"task_id\", \"unknown\")\n",
    "    latex_expression = json_data.get(\"latex_expression\")\n",
    "    test_cases = json_data.get(\"test_cases\", [])\n",
    "\n",
    "    if not latex_expression:\n",
    "        logger.warning(f\"Task ID {task_id}: No LaTeX expression provided\")\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No LaTeX expression provided\"}]\n",
    "\n",
    "    if not test_cases:\n",
    "        logger.warning(f\"Task ID {task_id}: No test cases provided\")\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": \"Error: No test cases provided\"}]\n",
    "\n",
    "    variables = extract_variables(test_cases[0].get(\"input\", {}))\n",
    "    python_code, error = generate_python_code(pipe, latex_expression, variables)\n",
    "    if error:\n",
    "        logger.error(f\"Task ID {task_id}: {error}\")\n",
    "        return task_id, [{\"latex_expression\": latex_expression, \"error\": error}]\n",
    "\n",
    "    return task_id, [{\"latex_expression\": latex_expression, \"generated_code\": python_code}]\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    try:\n",
    "        model_name = './fine_tuned_aimo_lora_model_v3'\n",
    "        # Load the model\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, tokenizer='./full_latest_72_tokenizer',torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        pipe.model.eval()\n",
    "        logger.info(\"Model loaded and evaluation mode set.\")\n",
    "\n",
    "        # Load JSON data\n",
    "        json_data_list = load_json_file(input_file)\n",
    "\n",
    "        # Initialize a dictionary to store results\n",
    "        results_dict = {\n",
    "            \"id\": [],\n",
    "            \"latex_expression\": [],\n",
    "            \"generated_code\": [],\n",
    "            \"error\": []\n",
    "        }\n",
    "\n",
    "        # Process each JSON object and store results in the dictionary\n",
    "        for json_data in tqdm(json_data_list):\n",
    "            task_id, results = process_json_data(pipe, json_data)\n",
    "            for result in results:\n",
    "                results_dict[\"id\"].append(task_id)\n",
    "                results_dict[\"latex_expression\"].append(result.get(\"latex_expression\", \"\"))\n",
    "                results_dict[\"generated_code\"].append(result.get(\"generated_code\", \"\"))\n",
    "                results_dict[\"error\"].append(result.get(\"error\", \"\"))\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "        df = pd.DataFrame(results_dict)\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        logger.info(f\"Results have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "input_file = 'public_test_new_no_sol_no_out.json'\n",
    "output_file = 'codes_26_2.csv'\n",
    "main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db89b7-0dbf-45bf-b462-164e21377894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df81b2b-af6d-4a88-a415-530b98870576",
   "metadata": {},
   "source": [
    "# Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498b8c7-ca41-45fa-bfef-60103f5326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def execute_python_code(code, input_values):\n",
    "    try:\n",
    "        # Create a dedicated namespace for the code execution\n",
    "        code_namespace = {}\n",
    "        exec(code, code_namespace)\n",
    "        \n",
    "        # Ensure the function name matches and exists in the namespace\n",
    "        if 'calculate' not in code_namespace:\n",
    "            return \"Error: Function 'calculate' not found in the generated code.\"\n",
    "        \n",
    "        # Execute the function with the provided input values\n",
    "        result = code_namespace['calculate'](**input_values)\n",
    "        \n",
    "        # Handle the result type\n",
    "        if isinstance(result, str):\n",
    "            result = eval(result)\n",
    "        if isinstance(result, complex):\n",
    "            return f\"{result.real}+{result.imag}j\"\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def main(json_file, csv_file, output_file):\n",
    "    try:\n",
    "        # Load the JSON data\n",
    "        json_data_list = load_json_file(json_file)\n",
    "        \n",
    "        # Load the CSV file containing generated code\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Initialize a dictionary to store the final results\n",
    "        results_dict = {\n",
    "            \"id\": [],\n",
    "            \"outputs\": []\n",
    "        }\n",
    "\n",
    "        # Iterate over each row in the CSV file\n",
    "        for _, row in df.iterrows():\n",
    "            task_id = row['id']\n",
    "            python_code = row['generated_code']\n",
    "            \n",
    "            # Find the corresponding JSON object\n",
    "            json_data = next((item for item in json_data_list if item['task_id'] == task_id), None)\n",
    "            if json_data is None:\n",
    "                print(f\"No JSON data found for task_id: {task_id}\")\n",
    "                continue\n",
    "\n",
    "            test_cases = json_data.get(\"test_cases\", [])\n",
    "            outputs = []\n",
    "            for case in test_cases:\n",
    "                input_values = case.get(\"input\", {})\n",
    "\n",
    "\n",
    "\n",
    "                # Execute the Python code with modified input values\n",
    "                result = execute_python_code(python_code,input_values)\n",
    "                outputs.append(result)\n",
    "\n",
    "\n",
    "            # Store the results\n",
    "            results_dict[\"id\"].append(task_id)\n",
    "            results_dict[\"outputs\"].append(outputs)\n",
    "\n",
    "        # Convert the results to a DataFrame\n",
    "        output_df = pd.DataFrame(results_dict)\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        output_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Outputs have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# File paths\n",
    "json_file = 'public_test_new_no_sol_no_out.json'\n",
    "csv_file = 'codes_test.csv'\n",
    "output_file = 'results.csv'\n",
    "\n",
    "# Run the main function\n",
    "main(json_file, csv_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17766d1c-7bd7-4f1a-9c85-39ab5e24fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89578f3f-718f-48a3-9ba4-65fef43de9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        sys.exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def extract_variables(input_values):\n",
    "    filtered_keys = [key for key in input_values.keys() if not key.endswith('_val')]\n",
    "    return filtered_keys\n",
    "\n",
    "def generate_python_code(pipe, latex_expression, variables):\n",
    "    prompt = f\"\"\"Convert this LaTeX expression: {latex_expression} to a Python function named 'calculate' that takes the following parameters: {', '.join(variables)}.\n",
    "The function must accept the variables provided in this prompt as parameters and must be callable using the `exec` function.\n",
    "\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Import necessary libraries and functions at the beginning of the function. Prefer using `sympy` for mathematical operations. Specially of log.\n",
    "2. For differentiation, integration, and other calculus operations, use `SymPy` to solve the equation and return a numerical value.\n",
    "3. Do not include any example usage of the function.\n",
    "4. Only provide the Python function, nothing else.\n",
    "5. The return type must be `int` or `float` and convert sympy complex number outputs into python complex number before returmning. Use `eval` or `evalf` only if the output type is not numeric.\n",
    "6. Keep the variable names (parameters) the same as provided. Carefully check or typecast variables where necessary. \n",
    "7. **Do not use `sp.lambdify` to generate the function.** Instead, directly define the function using standard Python and SymPy operations.\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 2000,\n",
    "        \"do_sample\": False,\n",
    "        \"stop_strings\": [\"```output\"],\n",
    "        \"tokenizer\": pipe.tokenizer,\n",
    "    }\n",
    "    try:\n",
    "        outputs = pipe(prompt, **gen_config)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        python_code = re.findall(r\"```python(.*?)```\", generated_text, re.DOTALL)\n",
    "        if not python_code:\n",
    "            return None, \"No Python code generated\"\n",
    "        return python_code[0].strip(), None\n",
    "    except Exception as e:\n",
    "        return None, f\"Error generating Python code: {str(e)}\"\n",
    "\n",
    "def process_data(example, pipe):\n",
    "    task_id = example.get(\"task_id\", \"unknown\")\n",
    "    latex_expression = example.get(\"latex_expression\")\n",
    "    test_cases = example.get(\"test_cases\", [])\n",
    "\n",
    "    if not latex_expression:\n",
    "        return {\n",
    "            \"id\": task_id,\n",
    "            \"latex_expression\": latex_expression,\n",
    "            \"generated_code\": None,\n",
    "            \"error\": \"Error: No LaTeX expression provided\"\n",
    "        }\n",
    "\n",
    "    if not test_cases:\n",
    "        return {\n",
    "            \"id\": task_id,\n",
    "            \"latex_expression\": latex_expression,\n",
    "            \"generated_code\": None,\n",
    "            \"error\": \"Error: No test cases provided\"\n",
    "        }\n",
    "\n",
    "    variables = extract_variables(test_cases[0].get(\"input\", {}))\n",
    "    python_code, error = generate_python_code(pipe, latex_expression, variables)\n",
    "    if error:\n",
    "        return {\n",
    "            \"id\": task_id,\n",
    "            \"latex_expression\": latex_expression,\n",
    "            \"generated_code\": None,\n",
    "            \"error\": error\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"id\": task_id,\n",
    "        \"latex_expression\": latex_expression,\n",
    "        \"generated_code\": python_code,\n",
    "        \"error\": None\n",
    "    }\n",
    "\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    try:\n",
    "        model_name = './fine_tuned_aimo_lora_model_v3'\n",
    "        tokenizer = AutoTokenizer.from_pretrained('AI-MO/NuminaMath-7B-TIR')\n",
    "\n",
    "        # Load the model\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        pipe.model.eval()\n",
    "\n",
    "        # Load JSON data\n",
    "        json_data_list = load_json_file(input_file)\n",
    "\n",
    "        # Convert list of dicts to dict of lists for Dataset\n",
    "        data_dict = {\n",
    "            \"id\": [],\n",
    "            \"latex_expression\": [],\n",
    "            \"generated_code\": [],\n",
    "            \"error\": []\n",
    "        }\n",
    "\n",
    "        # Process each JSON object and store results in the dictionary\n",
    "        for json_data in tqdm(json_data_list):\n",
    "            task_id, results = process_data(pipe, json_data)\n",
    "            for result in results:\n",
    "                data_dict[\"id\"].append(task_id)\n",
    "                data_dict[\"latex_expression\"].append(result.get(\"latex_expression\", \"\"))\n",
    "                data_dict[\"generated_code\"].append(result.get(\"generated_code\", \"\"))\n",
    "                data_dict[\"error\"].append(result.get(\"error\", \"\"))\n",
    "\n",
    "        # Create Dataset from dict of lists\n",
    "        dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "        # Convert the Dataset to a DataFrame\n",
    "        df = pd.DataFrame(data_dict)\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Results have been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "input_file = 'public_test_new_no_sol_no_out.json'\n",
    "output_file = 'codes_24_2.csv'\n",
    "main(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a6be2-2980-482c-b66c-ad176b273a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (llm_bootcamp)",
   "language": "python",
   "name": "llm_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
